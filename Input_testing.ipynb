{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suhail372/Hate_speech_detection_using_ML/blob/main/Input_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jicAamXYDANp"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Text Pre-processing libraries\n",
        "import nltk\n",
        "import string\n",
        "import warnings\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Tensorflow imports to build the model.\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuations(text):\n",
        "    punctuations_list = string.punctuation\n",
        "    temp = str.maketrans('', '', punctuations_list)\n",
        "    return text.translate(temp)"
      ],
      "metadata": {
        "id": "qLjzreLvDa6K"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s=input()\n",
        "s=remove_punctuations(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxVQ3QPEDffp",
        "outputId": "4bfdf286-9f2a-4311-caf6-774ada970cb0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You ever fuck a bitch and she start to cry? You be confused as shit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "\tstop_words = stopwords.words('english')\n",
        "\n",
        "\timp_words = []\n",
        "\n",
        "\t# Storing the important words\n",
        "\tfor word in str(text).split():\n",
        "\n",
        "\t\tif word not in stop_words:\n",
        "\n",
        "\t\t\t# Let's Lemmatize the word as well\n",
        "\t\t\t# before appending to the imp_words list.\n",
        "\n",
        "\t\t\tlemmatizer = WordNetLemmatizer()\n",
        "\t\t\tlemmatizer.lemmatize(word)\n",
        "\n",
        "\t\t\timp_words.append(word)\n",
        "\n",
        "\toutput = \" \".join(imp_words)\n",
        "\n",
        "\treturn output"
      ],
      "metadata": {
        "id": "uAezmA_KDtnD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s=remove_stopwords(s)\n",
        "X_train=s"
      ],
      "metadata": {
        "id": "8tYAivudDus9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Load the tokenizer from the saved file\n",
        "with open('tokenizer.pkl', 'rb') as tokenizer_file:\n",
        "    tokenizer = pickle.load(tokenizer_file)\n",
        "Training_seq=input_sequence = tokenizer.texts_to_sequences([s])\n",
        "Training_pad = pad_sequences(Training_seq,\n",
        "\t\t\t\t\t\t\tmaxlen=100,\n",
        "\t\t\t\t\t\t\tpadding='post',\n",
        "\t\t\t\t\t\t\ttruncating='post')"
      ],
      "metadata": {
        "id": "C_sedBj3Dx-I"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = keras.models.load_model(\"model.h5\")"
      ],
      "metadata": {
        "id": "TKMJHqTQEM-M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions=loaded_model.predict(Training_pad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf9G2x0AE1p8",
        "outputId": "eaff0bc2-f0a2-48d2-9d03-bddbf720312d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 975ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.argmax(predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYEq7IdoG3NY",
        "outputId": "e4aad4b1-7339-4a4b-e519-20d6845943a9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    }
  ]
}